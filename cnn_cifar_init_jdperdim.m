function net = cnn_cifar_init_jdperdim(varargin)
opts.networkType = 'simplenn' ;
opts.sparseStart = false; 
opts = vl_argparse(opts, varargin) ;

lr = [.1 2] ;

% Define network CIFAR10-quick    %layers初始化
net.layers = {} ;

% Block 1
net.layers{end+1} = struct('name','conv1','type', 'conv', ...
                           'weights', {{0.01*randn(5,5,3,32, 'single'), zeros(1, 32, 'single')}}, ...
                           'learningRate', lr, ...
                           'stride', 1, ...
                           'pad', 2) ;
net.layers{end+1} = struct('name','relu1','type', 'relu') ;                       
net.layers{end+1} = struct('name','mpool1','type', 'pool', ...
                           'method', 'max', ...
                           'pool', [3 3], ...
                           'stride', 2, ...
                           'pad', [0 1 0 1]) ;

% Block 2
net.layers{end+1} = struct('name','conv2','type', 'conv', ...
                           'weights', {{0.05*randn(5,5,32,32, 'single'), zeros(1,32,'single')}}, ...
                           'learningRate', lr, ...
                           'stride', 1, ...
                           'pad', 2) ;
net.layers{end+1} = struct('name','relu2','type', 'relu') ;
net.layers{end+1} = struct('name','apool2','type', 'pool', ...
                           'method', 'avg', ...
                           'pool', [3 3], ...
                           'stride', 2, ...
                           'pad', [0 1 0 1]) ; % Emulate caffe

% Block 3
net.layers{end+1} = struct('name','conv3','type', 'conv', ...
                           'weights', {{0.05*randn(5,5,32,32, 'single'), zeros(1,32,'single')}}, ...
                           'learningRate', lr, ...
                           'stride', 1, ...
                           'pad', 2) ;
net.layers{end+1} = struct('name','relu3','type', 'relu') ;
net.layers{end+1} = struct('name','apool3','type', 'pool', ...
                           'method', 'avg', ...
                           'pool', [3 3], ...
                           'stride', 2, ...
                           'pad', [0 1 0 1]) ; % Emulate caffe

% Block 4
net.layers{end+1} = struct('name','fc4','type', 'conv', ...
                           'weights', {{0.05*randn(4,4,32,500, 'single'), zeros(1,500,'single')}}, ...
                           'learningRate', lr, ...
                           'stride', 1, ...
                           'pad', 0) ;
net.layers{end+1} = struct('name','relu4','type', 'relu') ;

% Block 5
net.layers{end+1} = struct('name','fc5','type', 'conv', ...
                           'weights', {{0.05*randn(1,1,500,10, 'single'), zeros(1,10,'single')}}, ...
                           'learningRate', .1*lr, ...
                           'stride', 1, ...
                           'pad', 0) ;

% Loss layer
net.layers{end+1} = struct('name','softmaxloss','type', 'softmaxloss') ;   

% Meta parameters                 %meta初始化
net.meta.inputSize = [32 32 3] ;
net.meta.normalization = { }; % omit
net.meta.trainOpts.learningRate = [0.05*ones(1,15) 0.005*ones(1,10) 0.0005*ones(1,5)] ; % 30 10 5
net.meta.trainOpts.numEpochs = numel(net.meta.trainOpts.learningRate) ;
net.meta.trainOpts.batchSize = 100 ;
net.meta.trainOpts.weightDecay = 0.0000 ;

% Fill in default values
net = vl_simplenn_tidy(net) ;
vl_simplenn_display(net);

% Switch to DagNN if requested  %模型切换
switch lower(opts.networkType)
  case 'simplenn'
    % done
  case 'dagnn'
    net = dagnn.DagNN.fromSimpleNN(net, 'canonicalNames', true) ;
    net.addLayer('error', dagnn.Loss('loss', 'classerror'), ...
             {'prediction','label'}, 'error') ;
  otherwise
    assert(false) ;
end

